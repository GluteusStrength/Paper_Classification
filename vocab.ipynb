{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e02454f0-3127-4910-bfba-78e1754fbcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 vocab을 만드는 파일\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"final.csv\")\n",
    "dataset.dropna(inplace = True)\n",
    "dataset.reset_index(inplace = True)\n",
    "dataset.drop([\"index\"], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09739b0-ce5c-4885-b13c-a8ee64a1e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertWordPieceTokenizer를 통해서 일단 직접 tokenize를 진행\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    # 원래는 vocab이 들어가야 하지만 현재는 vocab을 만들어내야 하기 때문에 따로 지정하지 않는다.\n",
    "    None,\n",
    "    clean_text = True,\n",
    "    handle_chinese_chars = True,\n",
    "    strip_accents = False,\n",
    "    lowercase = False,\n",
    "    wordpieces_prefix = \"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494f28d3-93b6-4b08-b0da-b9573fb2bd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54242/54242 [00:00<00:00, 154568.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# encoder_input_train에는 tokenize하고자 하는 문장들이 들어가게 된다\n",
    "# ex) [\"나는 밥을 먹는다\", \"반찬은 고기다\", \"배가 불렀다\"] -> 해당 리스트가 encode 대상이다.\n",
    "encoder_input_train = []\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    encoder_input_train.append(dataset.loc[i, \"PAPER_TEXT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca6eb0f-205d-452a-b545-afa28c231e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encoder_input_train으로 구성한 텍스트셋을 학습을 통해서 vocab_size 크기 만큼 vocab을 만들어낸다.\n",
    "# 1차적으로 MyCanvas 파일의 vocab.txt는 8002개로 만들어냈다.\n",
    "# vocab_size = n -> n을 통해서 직접 본인이 원하는 tokenize를 진행할 수 있다.\n",
    "tokenizer.train_from_iterator(\n",
    "    encoder_input_train,\n",
    "    vocab_size=8002,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens = [\"[PAD]\", \"[CLS]\", \"[UNK]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b0a4766-6ab7-4a6a-b593-1f5d601f6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# txt 파일 디렉터리 및 파일명 지정\n",
    "vocab_dir = \"./vocab.txt\"\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocabulary = [[v, k] for k, v in vocab.items()]\n",
    "vocabulary = sorted(vocabulary)\n",
    "vocabulary = list(np.array(vocabulary)[:, 1])\n",
    "\n",
    "# vocabulary 저장\n",
    "with open(vocab_dir, 'w+') as lf:\n",
    "    lf.write('\\n'.join(vocabulary))\n",
    "    \n",
    "# 여기까지가 vocab.txt를 만들어내는 과정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30a20c35-08b1-4c86-bcf1-5a2b40b06c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1642: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BertTokenizer 확인하는 과정\n",
    "# 총 2가지의 Custom Berttokenizer를 만들어낼 수 있다.\n",
    "# 밑의 2가지 방법의 공통사항: do_basic_tokenize = False로 지정해야 한다.\n",
    "# 이는 우리가 하고자 하는 task에 그대로 똑같이 적용하면 된다.\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# bert_tokenizer = BertTokenizer(\"./vocab.txt\", do_basic_tokenize = False)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"./vocab.txt\", do_basic_tokenize = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
