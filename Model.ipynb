{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b07204b-82b0-4362-a0d3-2658c8805cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9609e2-f730-423b-b82a-dae800880212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 22 17:17:30 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.72       Driver Version: 410.72       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    32W / 250W |   2084MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb6ab8c-e4e5-434e-b673-41a40835a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터셋 마다 따로 조정해줄 필요가 있다.\n",
    "CFG = {\n",
    "    \"EPOCHS\": 50,\n",
    "    \"SEED\": 0,\n",
    "    \"learning_rate\": 4e-5,\n",
    "    \"batch_size\": 128\n",
    "}\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c1184c-72a1-4e62-a2fd-15b8fee4e48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PAPER_TEXT</th>\n",
       "      <th>target</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>대식세포이동저해인자 (Macrophage Migration Inhibitory Fa...</td>\n",
       "      <td>ND10</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>북방전복 (Haliotis discus hannai) 은 우리나라의 전복 양식생산량...</td>\n",
       "      <td>ND10</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tegillarca granosa is, ecologically warmwater ...</td>\n",
       "      <td>ND10</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matallothionein (MT) 은 약 60여 개의 아미노산으로 구성되고, 분...</td>\n",
       "      <td>ND10</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>굴 (Crassostrea gigas) 은 2015년 기준 세계 총생산량이 약 60...</td>\n",
       "      <td>ND10</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          PAPER_TEXT target  TARGET\n",
       "0  대식세포이동저해인자 (Macrophage Migration Inhibitory Fa...   ND10     260\n",
       "1  북방전복 (Haliotis discus hannai) 은 우리나라의 전복 양식생산량...   ND10     260\n",
       "2  Tegillarca granosa is, ecologically warmwater ...   ND10     260\n",
       "3  Matallothionein (MT) 은 약 60여 개의 아미노산으로 구성되고, 분...   ND10     260\n",
       "4  굴 (Crassostrea gigas) 은 2015년 기준 세계 총생산량이 약 60...   ND10     260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./final.csv\", encoding = 'utf-8')\n",
    "dataset.dropna(inplace = True)\n",
    "dataset.reset_index(inplace = True)\n",
    "dataset.drop([\"index\"], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcf9eb9-9705-4e3c-9fd6-3ad541c89599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54242 entries, 0 to 54241\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   PAPER_TEXT  54242 non-null  object\n",
      " 1   target      54242 non-null  object\n",
      " 2   TARGET      54242 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c1a981-a568-40cf-a3bc-3c0b0446a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선 class가 하나만 있는 것은 train에 다 포함시킬 것\n",
    "# 해당 셀은 class가 하나만 있는 것은 train에 포함시키기 전에 따로 떼어놓기 위함\n",
    "# stratify를 통해 균일하게 train과 valid를 분리하고자 한다.\n",
    "classes = dataset[\"TARGET\"].value_counts()\n",
    "classes = pd.DataFrame(classes)\n",
    "classes.reset_index(inplace = True)\n",
    "uniq_cls = list(classes[classes[\"count\"] == 1][\"TARGET\"])\n",
    "train_1 = []\n",
    "train_1_lbl = []\n",
    "for i in range(len(dataset)):\n",
    "    if dataset.loc[i, \"TARGET\"] in uniq_cls:\n",
    "        train_1.append(dataset.loc[i, :])\n",
    "        train_1_lbl.append(i)\n",
    "train_1 = pd.DataFrame(train_1)\n",
    "dataset.drop(train_1_lbl, axis = 0, inplace = True)\n",
    "dataset.reset_index(inplace = True)\n",
    "dataset.drop([\"index\"], axis = 1, inplace = True)\n",
    "train_1.reset_index(inplace = True)\n",
    "train_1.drop([\"index\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e05d431-b925-4acc-a3df-0fa60312eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 개수: 43386\n",
      "검증 데이터 개수: 10847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터를 train, validation 나누기\n",
    "train_ratio = 0.8\n",
    "validation_ratio = 0.2\n",
    "\n",
    "# train-validation 데이터를 먼저 나누고 나머지를 test 데이터로 나눕니다.\n",
    "train, valid = train_test_split(dataset, test_size = 1 - train_ratio, random_state=0, stratify = dataset[\"TARGET\"])\n",
    "\n",
    "# # 데이터 개수 확인\n",
    "print(\"학습 데이터 개수:\", len(train))\n",
    "print(\"검증 데이터 개수:\", len(valid))\n",
    "train = pd.concat([train_1, train], axis = 0)\n",
    "train.reset_index(inplace = True)\n",
    "train.drop([\"index\"], inplace = True, axis = 1)\n",
    "valid.reset_index(inplace = True)\n",
    "valid.drop([\"index\"], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c43653f-22f6-4e7c-988c-a19c810b8b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43395it [00:00, 862088.21it/s]\n",
      "10847it [00:00, 903389.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(mode):\n",
    "    data = None\n",
    "    if mode == \"train\":\n",
    "        data = train\n",
    "    if mode == \"valid\":\n",
    "        data = valid\n",
    "    if mode == \"test\":\n",
    "        data = test\n",
    "    mk_data = []\n",
    "    for sentence, label in tqdm(zip(data[\"PAPER_TEXT\"], data[\"TARGET\"])):\n",
    "        data_ = [sentence, label]\n",
    "        mk_data.append(data_)\n",
    "    return mk_data\n",
    "\n",
    "train_set = make_dataset(\"train\")\n",
    "valid_set = make_dataset(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76bf5c1a-1dff-4c66-808e-1f31ac6d56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTDataset 형태로 변경하는 과정\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair, mode = 'train'):\n",
    "        # bert_tokenizer: 텍스트를 토큰으로 분할하는 데 사용되는 토크나이저\n",
    "        # max_seq_length: 최대 시퀀스의 길이\n",
    "        # pad: 패딩 여부를 나타내는 boolean\n",
    "        # pair: 2개의 문장을 입력으로 받는 경우\n",
    "        # BERT 모델을 사용하는 자연어 처리 작업에서 데이터를 준비하고 변환하는 역할\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair, vocab = vocab)\n",
    "        self.mode = mode\n",
    "        if self.mode == \"train\":\n",
    "            self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "            self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "        else:\n",
    "            self.sentences = [transform(i) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.mode == \"train\":\n",
    "            return (self.sentences[i] + (self.labels[i], ))\n",
    "        else:\n",
    "            return self.sentences[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "756041d5-8705-4984-9f91-2751008f215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Hyperparameters\n",
    "# tokenizer를 통해서 우선 어느 정도의 max_len을 설정할 지 체크한다.\n",
    "# BERT모델의 입력 제한이 512\n",
    "max_len = 128\n",
    "batch_size = CFG[\"batch_size\"]\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = CFG[\"EPOCHS\"]\n",
    "# Gradient Clipping process\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = CFG[\"learning_rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f09a4af-836d-4923-8aa7-b07333338b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.vocab import Vocab\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import json\n",
    "# 이를 불러오고자 한다면 return_dict는 무조건 false로 해야한다.(transformers 버전 문제)\n",
    "bertmodel = BertModel.from_pretrained(\"skt/kobert-base-v1\", return_dict = False)\n",
    "# 커스텀 trained vocab txt 파일을 가지고 BertTokenzier 생성\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./vocab.txt\", do_basic_tokenize = False)\n",
    "tok = tokenizer.tokenize\n",
    "\n",
    "# customize vocab을 직접 불러온다.\n",
    "with open(\"vocab.txt\", \"r\") as f:\n",
    "    vocab = f.readlines()\n",
    "vocab = [voc.rstrip(\"\\n\") for voc in vocab]\n",
    "dic = dict()\n",
    "for i in range(len(vocab)):\n",
    "    dic[vocab[i]] = i\n",
    "# 최종적으로 vocab 형태를 만들어준다.\n",
    "vocab = Vocab(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eeffb2b-8afa-4e47-9805-71a4ca9beeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset, sent_idx: dataset의 0번은 embedding을 시켜야 할 문장, label_idx: 1번 index는 label에 해당, tok: tokenizer, max_len: 문장의 길이, True: padding 여부\n",
    "\"\"\"\n",
    "data_train = BERTDataset(dataset = train_set, sent_idx = 0, label_idx = 1, bert_tokenizer = tok, vocab = vocab, max_len = max_len, pad = True, pair = False)\n",
    "data_valid = BERTDataset(dataset = valid_set, sent_idx = 0, label_idx = 1, bert_tokenizer = tok, vocab = vocab, max_len = max_len, pad = True, pair = False)\n",
    "\n",
    "train_dataloader = DataLoader(data_train, batch_size = batch_size, num_workers = 4)\n",
    "valid_dataloader = DataLoader(data_valid, batch_size = batch_size, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "493a41ff-49b0-4d6a-89bd-fc09b4c105ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8005, 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# customize vocab을 이용하기 위함\n",
    "bertmodel.resize_token_embeddings(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31128fa7-ca31-4c47-8f93-3411a98d90a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 token: <unk>\n",
      "2번째 token: <pad>\n",
      "3번째 token: <bos>\n",
      "4번째 token: <eos>\n",
      "5번째 token: Eu\n",
      "6번째 token: ##ygen\n",
      "7번째 token: cause\n",
      "8번째 token: Each\n",
      "9번째 token: disper\n",
      "10번째 token: cho\n",
      "11번째 token: ##화로\n",
      "12번째 token: sources\n",
      "13번째 token: ##ashion\n",
      "14번째 token: 해석할\n",
      "15번째 token: 계산하였다\n",
      "16번째 token: 값의\n",
      "17번째 token: 연구결과를\n",
      "18번째 token: arg\n",
      "19번째 token: ##만의\n",
      "20번째 token: particle\n",
      "21번째 token: 결정하는\n",
      "22번째 token: ##inant\n",
      "23번째 token: brand\n",
      "24번째 token: 비교해\n",
      "25번째 token: 재생\n",
      "26번째 token: 아니다\n",
      "27번째 token: his\n",
      "28번째 token: ##위치\n",
      "29번째 token: ke\n",
      "30번째 token: meaning\n",
      "31번째 token: conventional\n",
      "32번째 token: ##ising\n",
      "33번째 token: ##by\n",
      "34번째 token: failure\n",
      "35번째 token: 단위로\n",
      "36번째 token: 구별\n",
      "37번째 token: draw\n",
      "38번째 token: ##으로서의\n",
      "39번째 token: recomm\n",
      "40번째 token: ##프라\n",
      "41번째 token: 화면\n",
      "42번째 token: sat\n",
      "43번째 token: get\n",
      "44번째 token: ess\n",
      "45번째 token: ##tual\n",
      "46번째 token: 퇴적\n",
      "47번째 token: 스테\n",
      "48번째 token: 작업을\n",
      "49번째 token: 경기\n",
      "50번째 token: 사람들이\n",
      "51번째 token: Ge\n",
      "52번째 token: 이루어졌\n",
      "53번째 token: ##cos\n",
      "54번째 token: Second\n",
      "55번째 token: ##화한\n",
      "56번째 token: calculation\n",
      "57번째 token: contain\n",
      "58번째 token: ##너지를\n",
      "59번째 token: ##된다고\n",
      "60번째 token: 노력이\n",
      "61번째 token: ANOVA\n",
      "62번째 token: 멘토\n",
      "63번째 token: priv\n",
      "64번째 token: 전문성\n",
      "65번째 token: approximately\n",
      "66번째 token: ##duce\n",
      "67번째 token: libr\n",
      "68번째 token: ##탄소\n",
      "69번째 token: 순위\n",
      "70번째 token: 높이는\n",
      "71번째 token: 효소\n",
      "72번째 token: 원활\n",
      "73번째 token: illustr\n",
      "74번째 token: ##모형의\n",
      "75번째 token: 개발하였다\n",
      "76번째 token: later\n",
      "77번째 token: ##term\n",
      "78번째 token: ##음에도\n",
      "79번째 token: 약간\n",
      "80번째 token: hal\n",
      "81번째 token: 단순한\n",
      "82번째 token: ##시스템의\n",
      "83번째 token: ##감은\n",
      "84번째 token: refer\n",
      "85번째 token: 년에는\n",
      "86번째 token: mill\n",
      "87번째 token: 적극적인\n",
      "88번째 token: solutions\n",
      "89번째 token: ##그룹\n",
      "90번째 token: 나타날\n",
      "91번째 token: invest\n",
      "92번째 token: fat\n",
      "93번째 token: XRD\n",
      "94번째 token: ##ercial\n",
      "95번째 token: 지리\n",
      "96번째 token: 수학교\n",
      "97번째 token: specimens\n",
      "98번째 token: ##럽게\n",
      "99번째 token: 활성을\n",
      "100번째 token: Disc\n",
      "101번째 token: importance\n",
      "102번째 token: ##연료\n",
      "103번째 token: optical\n",
      "104번째 token: 배제\n",
      "105번째 token: 성과를\n",
      "106번째 token: 구조의\n",
      "107번째 token: 강수\n",
      "108번째 token: 산림\n",
      "109번째 token: turn\n",
      "110번째 token: ##스는\n",
      "111번째 token: ##inking\n",
      "112번째 token: 유형에\n",
      "113번째 token: 생각을\n",
      "114번째 token: 플립\n",
      "115번째 token: emerg\n",
      "116번째 token: 유형을\n",
      "117번째 token: ##부는\n",
      "118번째 token: IR\n",
      "119번째 token: literature\n",
      "120번째 token: actual\n",
      "121번째 token: 도출하였다\n",
      "122번째 token: 탄성\n",
      "123번째 token: ##oud\n",
      "124번째 token: ##que\n",
      "125번째 token: likely\n",
      "126번째 token: vertical\n",
      "127번째 token: 선행연구를\n",
      "128번째 token: object\n",
      "label은 36\n"
     ]
    }
   ],
   "source": [
    "# vocab 파일과 매칭을 통하여 어떤식으로 tokenize가 되었는지를 확인하는 셀이다.\n",
    "# 직접 실행시켜보고 진행해도 좋다\n",
    "# data_train[0][0] -> data_train[데이터 index][input_ids의 index]\n",
    "voc_list = vocab.idx_to_token\n",
    "cnt = 1\n",
    "for i in data_train[0][0]:\n",
    "    print(\"{}번째 token: {}\".format(cnt, voc_list[cnt-1]))\n",
    "    cnt += 1\n",
    "print(\"label은 {}\".format(data_train[0][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1df1b51-65a9-4aee-8699-b55fa09cc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_size = 768, num_classes=365, dr_rate=None, params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "        torch.nn.init.kaiming_uniform(self.classifier.weight)\n",
    "    \n",
    "    def get_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.get_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76fa1678-3198-46c9-b16e-dbf141ab84a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "추후에 class imbalance를 해결하기 위한 방안 중, weighted cross entropy를 이용하고자 한다.\n",
    "\"\"\"\n",
    "target_unique = train.loc[:, \"TARGET\"].value_counts()\n",
    "target_unique = pd.DataFrame(target_unique)\n",
    "target_unique.loc[:, \"count\"]\n",
    "target_unique.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f71b7809-8607-4c0b-861d-a179b0c475db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate = 0.1).to(device)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "weighted = torch.tensor([1.0] * 365, device = device)\n",
    "for x in range(len(target_unique)):\n",
    "    target = target_unique.loc[x, \"TARGET\"]\n",
    "    count = target_unique.loc[x, \"count\"]\n",
    "    if count <= 150:\n",
    "        weighted[target] = 2.0\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weighted)\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6490c7e-f8c0-4569-91d9-8190c5339d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calc_accuracy(pred,label):\n",
    "    max_vals, max_indices = torch.max(pred, 1)\n",
    "    train_acc = (max_indices == label).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "\n",
    "def calc_f1_score(pred, label):\n",
    "    y_pred = torch.argmax(pred, dim = -1)\n",
    "    label = label.cpu().detach().numpy()\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    f1 = f1_score(label, y_pred, average = \"micro\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434ccc3-295d-4b94-bc96-9f5947f67684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 1 train f1 0.500390625\n",
      "epoch 1 train accuracy 0.500390625\n",
      "epoch 1 train loss 3.0883198096471673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 valid f1 0.1906762770897833\n",
      "epoch 1 valid accuracy 0.1906762770897833\n",
      "epoch 1 valid loss 4.608705307455624\n",
      "epoch 1 best valid loss 4.608705307455624\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 2 train f1 0.8025965073529412\n",
      "epoch 2 train accuracy 0.8025965073529412\n",
      "epoch 2 train loss 1.2337225429275456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 valid f1 0.18467395510835913\n",
      "epoch 2 valid accuracy 0.18467395510835913\n",
      "epoch 2 valid loss 4.962245638230268\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 3 train f1 0.8780330882352941\n",
      "epoch 3 train accuracy 0.8780330882352941\n",
      "epoch 3 train loss 0.7218527290531818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 valid f1 0.17700561145510837\n",
      "epoch 3 valid accuracy 0.17700561145510837\n",
      "epoch 3 valid loss 5.281047720067641\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 4 train f1 0.9067555147058823\n",
      "epoch 4 train accuracy 0.9067555147058823\n",
      "epoch 4 train loss 0.5031645392451216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 valid f1 0.182312306501548\n",
      "epoch 4 valid accuracy 0.182312306501548\n",
      "epoch 4 valid loss 5.421407738853904\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 5 train f1 0.9206801470588235\n",
      "epoch 5 train accuracy 0.9206801470588235\n",
      "epoch 5 train loss 0.3922219845137614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 valid f1 0.18017414860681114\n",
      "epoch 5 valid accuracy 0.18017414860681114\n",
      "epoch 5 valid loss 5.601568805470186\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:49<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 6 train f1 0.9328584558823529\n",
      "epoch 6 train accuracy 0.9328584558823529\n",
      "epoch 6 train loss 0.3155915729852174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 valid f1 0.17296923374613002\n",
      "epoch 6 valid accuracy 0.17296923374613002\n",
      "epoch 6 valid loss 5.946795216728659\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:49<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 7 train f1 0.9383042279411765\n",
      "epoch 7 train accuracy 0.9383042279411765\n",
      "epoch 7 train loss 0.2701249728910625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 valid f1 0.18525348297213623\n",
      "epoch 7 valid accuracy 0.18525348297213623\n",
      "epoch 7 valid loss 6.079554282917696\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 8 train f1 0.9448069852941177\n",
      "epoch 8 train accuracy 0.9448069852941177\n",
      "epoch 8 train loss 0.2333099403706215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 valid f1 0.18295568885448918\n",
      "epoch 8 valid accuracy 0.18295568885448918\n",
      "epoch 8 valid loss 6.161913473465863\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:49<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 9 train f1 0.9471737132352941\n",
      "epoch 9 train accuracy 0.9471737132352941\n",
      "epoch 9 train loss 0.21202269897467513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 valid f1 0.1831153250773994\n",
      "epoch 9 valid accuracy 0.1831153250773994\n",
      "epoch 9 valid loss 6.277985701841467\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:49<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 10 train f1 0.9539751838235294\n",
      "epoch 10 train accuracy 0.9539751838235294\n",
      "epoch 10 train loss 0.1818415974965319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 valid f1 0.1811174535603715\n",
      "epoch 10 valid accuracy 0.1811174535603715\n",
      "epoch 10 valid loss 6.384032810435576\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 11 train f1 0.9590533088235295\n",
      "epoch 11 train accuracy 0.9590533088235295\n",
      "epoch 11 train loss 0.15921705133137842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11 valid f1 0.17820820433436532\n",
      "epoch 11 valid accuracy 0.17820820433436532\n",
      "epoch 11 valid loss 6.420739863900577\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:49<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 12 train f1 0.9601792279411765\n",
      "epoch 12 train accuracy 0.9601792279411765\n",
      "epoch 12 train loss 0.14895111673069872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 valid f1 0.18090944272445822\n",
      "epoch 12 valid accuracy 0.18090944272445822\n",
      "epoch 12 valid loss 6.5383161713095275\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 13 train f1 0.9645220588235294\n",
      "epoch 13 train accuracy 0.9645220588235294\n",
      "epoch 13 train loss 0.1330773980661487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 valid f1 0.18774284055727555\n",
      "epoch 13 valid accuracy 0.18774284055727555\n",
      "epoch 13 valid loss 6.574300592085894\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:50<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 14 train f1 0.9650735294117647\n",
      "epoch 14 train accuracy 0.9650735294117647\n",
      "epoch 14 train loss 0.12733330144529806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 valid f1 0.1865441176470588\n",
      "epoch 14 valid accuracy 0.1865441176470588\n",
      "epoch 14 valid loss 6.732658341351677\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 340/340 [04:49<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "epoch 15 train f1 0.9682444852941177\n",
      "epoch 15 train accuracy 0.9682444852941177\n",
      "epoch 15 train loss 0.1154056545756141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 85/85 [00:24<00:00,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 valid f1 0.18525348297213623\n",
      "epoch 15 valid accuracy 0.18525348297213623\n",
      "epoch 15 valid loss 6.833712157081155\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████▌| 325/340 [04:38<00:12,  1.17it/s]"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = 999999\n",
    "train_loss_graph = []\n",
    "valid_loss_graph = []\n",
    "for e in range(num_epochs):\n",
    "    train_f1 = 0.0\n",
    "    train_loss = 0.0\n",
    "    valid_f1 = 0.0\n",
    "    valid_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length = valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()    # scheduler 적용\n",
    "        train_f1 += calc_f1_score(out, label)\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        train_loss += loss.item()\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"epoch {} train f1 {}\".format(e+1, train_f1 / (batch_id+1)))\n",
    "    print(\"epoch {} train accuracy {}\".format(e+1, train_acc / (batch_id + 1)))\n",
    "    print(\"epoch {} train loss {}\".format(e+1, train_loss / len(train_dataloader)))\n",
    "    train_loss_graph.append(train_loss / len(train_dataloader))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        valid_f1 += calc_f1_score(out, label)\n",
    "        valid_acc += calc_accuracy(out, label)\n",
    "        valid_loss += loss.item()\n",
    "    print(\"epoch {} valid f1 {}\".format(e+1, valid_f1 / (batch_id+1)))\n",
    "    print(\"epoch {} valid accuracy {}\".format(e+1, valid_acc / (batch_id + 1)))\n",
    "    # validation loss 설정\n",
    "    print(\"epoch {} valid loss {}\".format(e+1, valid_loss / len(valid_dataloader)))\n",
    "    val_loss = valid_loss / len(valid_dataloader)\n",
    "    valid_loss_graph.append(val_loss)\n",
    "    if best_loss > val_loss:\n",
    "        best_model = model\n",
    "        best_loss = val_loss\n",
    "        print(\"epoch {} best valid loss {}\".format(e+1, best_loss))\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "568f615d-0077-45c9-9a71-06331964f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'best_1020_version2.pt')\n",
    "torch.save(model, 'last_1020_version2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
